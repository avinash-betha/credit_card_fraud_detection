{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f988c8-3fc3-4fd7-ba3f-3f962a46fea5",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection \n",
    "\n",
    "This notebook demonstrates an end-to-end big data solution using Apache Spark on EMR. We:\n",
    "\n",
    "- Ingest raw transaction data from S3,\n",
    "- Clean and transform the data (feature engineering),\n",
    "- Save the transformed data in Parquet (for Athena integration),\n",
    "- Build a fraud detection model using Spark MLlib on the transformed data, and\n",
    "- Create visualizations to evaluate the model.\n",
    "\n",
    "Let's begin by initializing our Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53988204-5d66-48bc-953b-2c3d7b818f34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:07:47.175478Z",
     "iopub.status.busy": "2025-03-14T20:07:47.174256Z",
     "iopub.status.idle": "2025-03-14T20:08:37.061009Z",
     "shell.execute_reply": "2025-03-14T20:08:37.060366Z",
     "shell.execute_reply.started": "2025-03-14T20:07:47.174657Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698796438186431e836e20cfd62dfb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1741982580669_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-90-6.ec2.internal:20888/proxy/application_1741982580669_0001/\" class=\"emr-proxy-link j-2W5Q4XO46BPA2 application_1741982580669_0001\" emr-resource=\"j-2W5Q4XO46BPA2\n",
       "\" application-id=\"application_1741982580669_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-93-33.ec2.internal:8042/node/containerlogs/container_1741982580669_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession started successfully!"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession with required S3 configurations.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionDataProcessing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession started successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6caf24-3c46-4f6a-aa0a-d4de2ece7ca6",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "loading the raw CSV data from S3. This dataset (Synthetic_Financial_datasets_log.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfbc09d-f822-4ddb-b6ee-c88346e0d079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:09:50.968104Z",
     "iopub.status.busy": "2025-03-14T20:09:50.967798Z",
     "iopub.status.idle": "2025-03-14T20:10:16.431178Z",
     "shell.execute_reply": "2025-03-14T20:10:16.430411Z",
     "shell.execute_reply.started": "2025-03-14T20:09:50.968073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88763af8d81d4d80b963a98aebc35e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Define the S3 path to your dataset.\n",
    "raw_data_path = \"s3://projectfraudcredit/Synthetic_Financial_datasets_log.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame with header and inferred schema.\n",
    "df = spark.read.csv(raw_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Preview the initial data.\n",
    "print(\"Initial Data:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54b8d6-6ec5-41db-896b-59506df9be7a",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "We remove duplicate rows, filter out rows with null values in critical columns (e.g., 'amount'), and add a unique transaction identifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4afb22-d94d-4a79-aeff-dd22c8d71906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:11:02.498070Z",
     "iopub.status.busy": "2025-03-14T20:11:02.497678Z",
     "iopub.status.idle": "2025-03-14T20:11:17.861753Z",
     "shell.execute_reply": "2025-03-14T20:11:17.860741Z",
     "shell.execute_reply.started": "2025-03-14T20:11:02.498024Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a2edf3c9134cfd83ddbd17478d193b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data:\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|transaction_id|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+\n",
      "|  20|CASH_IN|147268.71|C2004857446|      18817.0|     166085.71|  C40874062|           0.0|           0.0|      0|             0|             0|\n",
      "|  20|CASH_IN| 82455.73|C1240676145|   6689707.73|    6772163.45|C1984982364|    2053118.45|    1970662.72|      0|             0|             1|\n",
      "|  20|PAYMENT| 17832.11| C521375485|      30304.0|      12471.89|M1921757665|           0.0|           0.0|      0|             0|             2|\n",
      "|  20|CASH_IN| 70493.83|C2057775424|      6703.71|      77197.54| C374264195|      97419.46|      26925.63|      0|             0|             3|\n",
      "|  20|  DEBIT|   7816.6|C1756648408|          0.0|           0.0|  C57497207|    1943605.47|    1951422.06|      0|             0|             4|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "# Drop duplicate rows across all columns.\n",
    "df_cleaned = df.dropDuplicates()\n",
    "\n",
    "# Filter out rows where 'amount' is null.\n",
    "df_cleaned = df_cleaned.filter(col(\"amount\").isNotNull())\n",
    "\n",
    "# Create a unique transaction identifier.\n",
    "df_cleaned = df_cleaned.withColumn(\"transaction_id\", monotonically_increasing_id())\n",
    "\n",
    "# Preview the cleaned data.\n",
    "print(\"Cleaned Data:\")\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd65900-fd0f-484e-94cf-2bf02ffcf21a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We compute additional features using window functions. In this example, we aggregate by the originator (nameOrig) to calculate:\n",
    "- Transaction frequency,\n",
    "- Total, average, and standard deviation of the amount, and\n",
    "- The difference between the old and new balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8dcaafe-cbc7-45cc-ab1d-2c569b1629f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:14:30.658824Z",
     "iopub.status.busy": "2025-03-14T20:14:30.658570Z",
     "iopub.status.idle": "2025-03-14T20:15:00.191177Z",
     "shell.execute_reply": "2025-03-14T20:15:00.190294Z",
     "shell.execute_reply.started": "2025-03-14T20:14:30.658796Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eee1358359946c49ec247e678d80d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with Engineered Features:\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+---------------------+------------+----------+--------------+-------------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|transaction_id|transaction_frequency|total_amount|avg_amount|std_dev_amount|       balance_diff|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+---------------------+------------+----------+--------------+-------------------+\n",
      "| 385| CASH_IN|139018.58|C1000021278|   4886789.95|    5025808.54| C174750132|     411015.63|     271997.05|      0|             0|  120259328329|                    1|   139018.58| 139018.58|          NULL|-139018.58999999985|\n",
      "| 352|CASH_OUT|406150.63|C1000036439|      49511.0|           0.0| C382608685|           0.0|     406150.63|      0|             0|  163208932829|                    1|   406150.63| 406150.63|          NULL|            49511.0|\n",
      "| 324|CASH_OUT|489569.69|C1000037068|          0.0|           0.0|C1439480491|     932646.56|    1422216.25|      0|             0|   42949820055|                    1|   489569.69| 489569.69|          NULL|                0.0|\n",
      "| 274| PAYMENT|  4373.25|C1000040348|       4145.0|           0.0|M2086754197|           0.0|           0.0|      0|             0|   94489417494|                    1|     4373.25|   4373.25|          NULL|             4145.0|\n",
      "| 393|CASH_OUT| 61997.99|C1000048932|        130.0|           0.0| C655566527|           0.0|      61997.99|      0|             0|  180388841705|                    1|    61997.99|  61997.99|          NULL|              130.0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+---------------------+------------+----------+--------------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum as _sum, avg, stddev, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window partitioned by the originator (nameOrig)\n",
    "windowSpec = Window.partitionBy(\"nameOrig\")\n",
    "\n",
    "# Compute new features.\n",
    "df_features = df_cleaned.withColumn(\"transaction_frequency\", count(\"transaction_id\").over(windowSpec)) \\\n",
    "    .withColumn(\"total_amount\", _sum(\"amount\").over(windowSpec)) \\\n",
    "    .withColumn(\"avg_amount\", avg(\"amount\").over(windowSpec)) \\\n",
    "    .withColumn(\"std_dev_amount\", stddev(\"amount\").over(windowSpec)) \\\n",
    "    .withColumn(\"balance_diff\", expr(\"oldbalanceOrg - newbalanceOrig\"))\n",
    "\n",
    "# Preview the DataFrame with engineered features.\n",
    "print(\"Data with Engineered Features:\")\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4c5bc-37b6-4b96-b5e1-fbadb48c33c3",
   "metadata": {},
   "source": [
    "# Saving Transformed Data\n",
    "\n",
    "We save the transformed DataFrame in Parquet format to S3. This data is used to create an Athena external table for efficient querying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf82df4-da73-4c3d-9aff-e34b25ed5d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:19:19.291039Z",
     "iopub.status.busy": "2025-03-14T20:19:19.290805Z",
     "iopub.status.idle": "2025-03-14T20:20:10.819098Z",
     "shell.execute_reply": "2025-03-14T20:20:10.817883Z",
     "shell.execute_reply.started": "2025-03-14T20:19:19.291013Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf41c37c6754ea48e23e98460175065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data successfully saved to: s3://projectfraudcredit/transformed-data/\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+---------------------+------------+----------+--------------+------------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|transaction_id|transaction_frequency|total_amount|avg_amount|std_dev_amount|      balance_diff|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+---------------------+------------+----------+--------------+------------------+\n",
      "|  12|CASH_OUT|367527.28|C1000012640|          0.0|           0.0|C1239707538|    2411831.94|     2824848.7|      0|             0|   25769839836|                    1|   367527.28| 367527.28|          NULL|               0.0|\n",
      "| 204| CASH_IN|263258.49| C100004036|     134762.0|     398020.49| C495358645|           0.0|           0.0|      0|             0|  163208838531|                    1|   263258.49| 263258.49|          NULL|        -263258.49|\n",
      "| 610| PAYMENT|   677.55|C1000042362|          0.0|           0.0| M203847020|           0.0|           0.0|      0|             0|   85899586330|                    1|      677.55|    677.55|          NULL|               0.0|\n",
      "| 232| CASH_IN|119410.85|C1000056044|    819841.58|     939252.43| C822758946|     187967.25|      68556.39|      0|             0|  214748401784|                    1|   119410.85| 119410.85|          NULL|-119410.8500000001|\n",
      "| 402|CASH_OUT|159128.23|C1000057056|          0.0|           0.0| C244386981|     298559.66|     457687.89|      0|             0|   17180113997|                    1|   159128.23| 159128.23|          NULL|               0.0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+--------------+---------------------+------------+----------+--------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Define the S3 path where the transformed data will be saved.\n",
    "transformed_data_path = \"s3://projectfraudcredit/transformed-data/\"\n",
    "\n",
    "# Write the DataFrame as Parquet, overwriting any existing data.\n",
    "df_features.write.mode(\"overwrite\").parquet(transformed_data_path)\n",
    "\n",
    "print(\"Transformed data successfully saved to:\", transformed_data_path)\n",
    "\n",
    "# To simulate Athena integration, we can read the Parquet data back.\n",
    "df_features = spark.read.parquet(transformed_data_path)\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9931b392-1372-4a13-8287-7427c62a77b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:21:05.888222Z",
     "iopub.status.busy": "2025-03-14T20:21:05.887974Z",
     "iopub.status.idle": "2025-03-14T20:21:13.196017Z",
     "shell.execute_reply": "2025-03-14T20:21:13.195180Z",
     "shell.execute_reply.started": "2025-03-14T20:21:05.888196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccab6ca719e14a788fe08477314e7ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.37.13-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0\n",
      "  Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
      "Collecting botocore<1.38.0,>=1.37.13\n",
      "  Downloading botocore-1.37.13-py3-none-any.whl (13.4 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/lib/python3.9/site-packages (from botocore<1.38.0,>=1.37.13->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3.9/site-packages (from botocore<1.38.0,>=1.37.13->boto3) (1.25.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.13->boto3) (1.13.0)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.37.13 botocore-1.37.13 s3transfer-0.11.4\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41becbc-e632-479d-923c-4c478e794397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:21:33.494741Z",
     "iopub.status.busy": "2025-03-14T20:21:33.494507Z",
     "iopub.status.idle": "2025-03-14T20:21:52.855278Z",
     "shell.execute_reply": "2025-03-14T20:21:52.854435Z",
     "shell.execute_reply.started": "2025-03-14T20:21:33.494713Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff160ea1d91749b38d31f23fe3773fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.0.2\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./tmp/spark-e261b25f-5ad7-452a-8746-75e933d9a9ec/lib64/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.13.0)\n",
      "Installing collected packages: tzdata, python-dateutil, pandas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3.9/site-packages, outside environment /mnt/yarn/usercache/livy/appcache/application_1741982580669_0001/container_1741982580669_0001_01_000001/tmp/spark-e261b25f-5ad7-452a-8746-75e933d9a9ec\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "Successfully installed pandas-2.2.3 python-dateutil-2.9.0.post0 tzdata-2025.1\n",
      "\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./tmp/spark-e261b25f-5ad7-452a-8746-75e933d9a9ec/lib64/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib64/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 2.17.18 requires python-dateutil<=2.8.2,>=2.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"numpy\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c890b-069b-4e19-ae5a-76de9cec2851",
   "metadata": {},
   "source": [
    "# Machine Learning: Fraud Detection Model\n",
    "\n",
    "We assemble features into a vector, split the data into training and test sets, and build a Logistic Regression model using Spark MLlib. The label for fraud detection is assumed to be in the 'isFraud' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bafb784e-35f8-433b-9995-ad73f15fe1a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:22:01.086351Z",
     "iopub.status.busy": "2025-03-14T20:22:01.086113Z",
     "iopub.status.idle": "2025-03-14T20:23:02.731327Z",
     "shell.execute_reply": "2025-03-14T20:23:02.730231Z",
     "shell.execute_reply.started": "2025-03-14T20:22:01.086324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf3a22fa84f4850a3998f7f5fd64b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC: 0.9901976243035846\n",
      "Test Accuracy: 0.9994488333639537\n",
      "Test Precision: 0.9994491373720265\n",
      "Test Recall: 0.9994488333639537\n",
      "Test F1 Score: 0.9993737503002043\n",
      "Confusion Matrix:\n",
      "[[5436    0]\n",
      " [   3    4]]"
     ]
    }
   ],
   "source": [
    "# Importing necessary ML classes from Spark MLlib evaluators.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Model Training & Predictions\n",
    "\n",
    "# Defining the feature columns.\n",
    "feature_cols = [\"amount\", \"transaction_frequency\", \"total_amount\", \"avg_amount\", \"std_dev_amount\", \"balance_diff\"]\n",
    "\n",
    "# Creating the VectorAssembler with handleInvalid=\"skip\" to ignore rows with nulls in feature columns.\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "data = assembler.transform(df_features)\n",
    "\n",
    "# Filtering out rows where the label (isFraud) is null.\n",
    "data = data.filter(data.isFraud.isNotNull())\n",
    "\n",
    "# Splitting data into training (70%) and test (30%) sets.\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Initialize and train a Logistic Regression model.\n",
    "lr = LogisticRegression(labelCol=\"isFraud\", featuresCol=\"features\", maxIter=10)\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Making predictions on test data.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# ---------------------------\n",
    "# Model Evaluation using Spark Evaluators\n",
    "\n",
    "# Evaluating ROC-AUC using BinaryClassificationEvaluator.\n",
    "roc_evaluator = BinaryClassificationEvaluator(labelCol=\"isFraud\", metricName=\"areaUnderROC\")\n",
    "roc_auc = roc_evaluator.evaluate(predictions)\n",
    "print(\"Test ROC-AUC:\", roc_auc)\n",
    "\n",
    "# Evaluating accuracy, precision, recall, and F1 using MulticlassClassificationEvaluator.\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "print(\"Test Precision:\", precision)\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "print(\"Test Recall:\", recall)\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "print(\"Test F1 Score:\", f1_score)\n",
    "\n",
    "# ---------------------------\n",
    "# Confusion Matrix (using Pandas & scikit-learn)\n",
    "\n",
    "# Converting predictions DataFrame to Pandas for a detailed confusion matrix.\n",
    "preds_pd = predictions.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "# Import confusion_matrix from scikit-learn.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(preds_pd[\"isFraud\"], preds_pd[\"prediction\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bb206-5344-408e-a7f5-f3e80299fee0",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "We create visualizations to represent our model performance and key data distributions. Here, we will plot the ROC curve using Matplotlib and scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49614f06-b55e-4cd3-8a02-bc85d540a387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:23:14.321555Z",
     "iopub.status.busy": "2025-03-14T20:23:14.321322Z",
     "iopub.status.idle": "2025-03-14T20:23:43.719436Z",
     "shell.execute_reply": "2025-03-14T20:23:43.718043Z",
     "shell.execute_reply.started": "2025-03-14T20:23:14.321529Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff8a55c2c54538b1ea7ef1b8ce20e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dateutil==2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil==2.8.2) (1.13.0)\n",
      "Installing collected packages: python-dateutil\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "Successfully installed python-dateutil-2.8.2\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"python-dateutil==2.8.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24f46ea-c3f0-4182-abda-f617f2a3397b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:24:10.116159Z",
     "iopub.status.busy": "2025-03-14T20:24:10.115918Z",
     "iopub.status.idle": "2025-03-14T20:24:25.481483Z",
     "shell.execute_reply": "2025-03-14T20:24:25.480277Z",
     "shell.execute_reply.started": "2025-03-14T20:24:10.116133Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afe907acc744e12ae28c66663816445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./tmp/spark-e261b25f-5ad7-452a-8746-75e933d9a9ec/lib64/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tmp/spark-e261b25f-5ad7-452a-8746-75e933d9a9ec/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.13.0)\n",
      "Installing collected packages: zipp, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.56.0 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.1.0 zipp-3.21.0\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b5c0f0-e969-4a43-a3f4-52603b3e3934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T20:39:21.001824Z",
     "iopub.status.busy": "2025-03-14T20:39:21.001593Z",
     "iopub.status.idle": "2025-03-14T20:39:38.340589Z",
     "shell.execute_reply": "2025-03-14T20:39:38.339860Z",
     "shell.execute_reply.started": "2025-03-14T20:39:21.001797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e148bcd03b9546c1a4b04c7d85a054dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot uploaded to: s3://projectfraudcredit/plots/roc_curve.png\n",
      "Plot uploaded to: s3://projectfraudcredit/plots/confusion_matrix.png\n",
      "Plot uploaded to: s3://projectfraudcredit/plots/feature_coefficients.png"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "import boto3\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "\n",
    "s3_bucket = \"projectfraudcredit\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "def upload_plot_to_s3(s3_key):\n",
    "    \"\"\"\n",
    "    Uploads the current matplotlib figure directly to S3 using an in-memory buffer.\n",
    "    \"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    s3_client.upload_fileobj(buf, s3_bucket, s3_key)\n",
    "    print(f\"Plot uploaded to: s3://{s3_bucket}/{s3_key}\")\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Prepare Data for Visualizations\n",
    "# Convert predictions to a Pandas DataFrame and extract the probability for class 1.\n",
    "preds_pd = predictions.select(\"isFraud\", \"prediction\", \"probability\").toPandas()\n",
    "preds_pd[\"probability_class1\"] = preds_pd[\"probability\"].apply(lambda x: x[1] if isinstance(x, (list, tuple)) else x[1])\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. ROC Curve Visualization\n",
    "fpr, tpr, thresholds = roc_curve(preds_pd[\"isFraud\"], preds_pd[\"probability_class1\"])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='best')\n",
    "upload_plot_to_s3(\"plots/roc_curve.png\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Confusion Matrix Visualization\n",
    "cm = confusion_matrix(preds_pd[\"isFraud\"], preds_pd[\"prediction\"])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.matshow(cm, cmap=plt.cm.Blues, fignum=1)\n",
    "plt.title('Confusion Matrix', pad=20)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "# Annotate each cell with its count.\n",
    "for (i, j), value in np.ndenumerate(cm):\n",
    "    plt.text(j, i, f'{value}', ha='center', va='center', color='red')\n",
    "upload_plot_to_s3(\"plots/confusion_matrix.png\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Logistic Regression Feature Coefficients Visualization\n",
    "coefficients = model.coefficients.toArray()\n",
    "feature_names = feature_cols\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(feature_names, coefficients)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Feature Coefficients')\n",
    "plt.xticks(rotation=45)\n",
    "upload_plot_to_s3(\"plots/feature_coefficients.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca428185-4fb4-40f4-ab84-58b5c0a38ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
